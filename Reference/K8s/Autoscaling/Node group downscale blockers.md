#k8s #autoscaling 

## ğŸ”¹ **Common Reasons for Node Downscale Block (Both Cluster Autoscaler & Karpenter)**

### 1. **Non-evictable pods (e.g., DaemonSets or pods with `local storage`)**
- **DaemonSets**: Run on every node and are not evicted during scale-down.
- **Pods with local storage** (e.g., `emptyDir`, `hostPath`): Eviction may cause data loss, so autoscalers avoid disrupting them.
- **Pods with `podDisruptionBudget` violations**: If a scale-down would violate a PDB, it's blocked.  This will always block if a PDB is set to 0.
- **Pods not managed by a controller** (like bare `kubectl run` pods): Theyâ€™re not safe to reschedule, so the node stays.
- `safe-to-evict` label is set to `false`

### 2. **Pods with node affinity or anti-affinity rules**
- If a pod can **only run** on specific node types (e.g., `requiredDuringSchedulingIgnoredDuringExecution`), the autoscaler wonâ€™t remove a node if itâ€™s the only place such pods can live.

### 3. **Resource fragmentation**
- If pods **canâ€™t be rescheduled elsewhere** due to resource constraints (CPU/mem), even if a node looks underutilized, it may stay online.
- Example: Node has 500m CPU left, but the only pod to schedule needs 600m.

### 4. **Pods in `Terminating` or `Unknown` state**
- Karpenter and Cluster Autoscaler will wait until terminating pods finish.
- Nodes hosting `Unknown` pods may be considered tainted and not downsized.


---

## ğŸ”¹ **Cluster Autoscaler-Specific Limitations**

### 5. **Minimum node group size**
- If the node groupâ€™s `minSize` is >0, CA won't go below itâ€”even if the node is empty.

### 6. **Node utilization thresholds not met**
- CA uses a **utilization threshold** (default 50% for CPU/mem). A node wonâ€™t be removed if itâ€™s considered â€œin use.â€

### 7. **Custom annotations preventing scale-down**
- `cluster-autoscaler.kubernetes.io/scale-down-disabled` on nodes or pods prevents downsizing.

---

## ğŸ”¹ **Karpenter-Specific Scenarios**

### 8. **TTL Settings Not Met**
- Karpenter uses `TTLSecondsAfterEmpty` and `TTLSecondsUntilExpired`. If these havenâ€™t elapsed, the node stays.

### 9. **Drift detection policies**
- If a node has drift (e.g., wrong AMI, taints, labels), Karpenter may keep or replace it instead of downsizing based on usage.

### 10. **Linked volumes or constraints**
- If a pod is using a volume thatâ€™s not reattachable or reschedulable (e.g., a `hostPath` volume), Karpenter canâ€™t drain the pod safely.