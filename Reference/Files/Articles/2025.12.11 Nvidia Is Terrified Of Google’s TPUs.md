#tpu #gpu #google #nvidia #ai 

Nvidia’s Q3 results are out and they are a banger. ==$57 billion in revenue which is up 62% year over year.== This is mostly from Nvidia’s data center business (so AI). Other sectors saw much more modest growth. So if there is an AI bubble it certainly hasn’t popped yet.

But is it possible for a company to be too successful? I think yes. You see, one of the noteworthy parts of the report was that Nvidia said how their datacenter GPUs were sold out. Now this could be a good thing if you have a monopoly, but Nvidia doesn’t have a monopoly. There is also AMD which makes data center GPUs.

However AMD only has a tiny piece of the AI market, about 10%. This appears to be mostly due to the CUDA ‘moat’. CUDA, or the Compute Unified Device Architecture, is NVIDIA’s parallel computing platform that allows software to communicate directly with the GPU. Nvidia has spent a lot of time developing CUDA, significantly more than AMD and their ROCm platform, and apparently AI companies love it. Virtually all major AI frameworks, like PyTorch and TensorFlow, are optimized to run on CUDA.

But it is possible to switch. In fact, I have no idea why more companies haven’t switched. AMD’s AI GPUs are significantly cheaper. ==How has the CUDA moat given Nvidia 90% market share?==

The only major group of companies that have quit Nvidia are Chinese companies due to [sanctions](https://medium.com/@impure/trump-really-wants-to-kill-the-cuda-moat-huh-ef65b04e3d28) and they appear to be doing fine.

![Meme comparing Chinese AI to American AI](https://miro.medium.com/v2/resize:fit:975/1*kyc4yJPHrhfnMN7HnFn4kw.png)

So why don’t US companies switch to AMD? ==I guess the cost difference is not big enough to justify rewriting a significant portion of their code.== Especially as a lot of the cost comes from the power the GPUs use.

So what if we introduced a third competitor? Some company that has dramatically decreased their energy costs? Well, that company may just have a shot at destroying the CUDA moat in the US. And that company is Google.

Google has been building their own tensor processing units (TPUs) for quite some time. A TPU is a type of ASIC, Application-specific integrated circuit. These are custom built chips that can do one task really well and really efficiently. However they’re not really good at doing anything else.

Google has historically kept their TPUs in-house. In order to use them you’d have to sign up to their Google Cloud Platform or GCP. But now, perhaps due to the problems of getting a Blackwell GPU, Meta is in talks with Google to buy some of their TPUs. And Nvidia in response posted this:

> We’re delighted by Google’s success — they’ve made great advances in AI and we continue to supply to Google.
> 
> NVIDIA is a generation ahead of the industry — it’s the only platform that runs every AI model and does it everywhere computing is done.
> 
> NVIDIA offers greater performance, versatility, and fungibility than ASICs, which are designed for specific AI frameworks or functions

The comments to that tweet are hilarious.

and

So the general sentiment is that Nvidia is terrified of this deal. And they should be.

==It’s not exactly clear how good TPUs are== because you can’t buy them yet, but just going by prices on GCP it looks like Google’s TPUs are significantly cheaper to run than Nvidia’s chips.

I suspect that this is what has allowed Google to constantly undercut the competition’s pricing on their Gemini 2.0 models. Although due to a lack of competition, Gemini 2.5 and now Gemini 3.0 are ticking up in price.

Now Nvidia says they’re a generation ahead. Is this true? Maybe. Maybe not. One [paper](https://introl.com/blog/google-tpu-v6e-vs-gpu-4x-better-ai-performance-per-dollar-guide) has shown that TPUs can actually outperform Nvidia’s GPUs while being significantly more efficient:

> Recent deployments showcase dramatic results: Midjourney reduced inference costs by 65% after migrating from GPUs, Cohere achieved 3x throughput improvements, and Google’s own Gemini models utilize tens of thousands of TPU chips for training.

There are more comparisons in that paper. And, of course, the efficiency argument is hard to ignore. If these things are 4x more efficient then you can hypothetically put 4x as many in a data center for the same power cost, vastly increasing your throughput.

The only thing is the upfront cost. How much do TPUs cost to buy? No one knows. But I suspect Google can price them very attractively. The reason is that Nvidia has a ridiculous markup on their GPUs. In my previous post I noted how it could be as high as [1000%](https://medium.com/@impure/nvidia-invested-100-billion-to-prolong-the-ai-bubble-f3d8ae2047f9).

Google likely has similar costs as all these chips are made by TSMC. So all Google would have to do is drop the price by like 50% and then they could say, “You could pay full price for CUDA or for half the price you could use TPUs instead.”

Lastly I should also note that Google recently announced their Ironwood TPUs. The [blog post](https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/) was a bit light on details, but it did show these two graphs:

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1500/1*rS4rlEzRtc8CA8fZlZNDOQ.jpeg)

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1500/1*KkzU8UdtwvJ1QuzxkvSLrQ.jpeg)

Generation ahead my ass

So I think Google is going to give Nvidia a run for its money. And about time too. ==I don’t understand how one company can have 90% market share in the AI field when their competitor has an equivalent product for a fraction of the cost.== I guess CUDA is just that good.

If Google starts selling their TPUs they would likely have to deal with the same problems AMD has been facing so it’s not guaranteed. But TPUs have already been shown to be quite popular in GCP, and perhaps selling them could make them even more popular.