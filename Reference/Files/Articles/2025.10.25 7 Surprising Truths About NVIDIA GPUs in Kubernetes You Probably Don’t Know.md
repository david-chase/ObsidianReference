#medium #k8s #gpu #nvidia

![](https://miro.medium.com/v2/resize:fit:821/1*cYhotf2cU-swUP1El3Se2A.png)

AI Generated Image

## Introduction: The GPU Illusion

The adoption of GPUs for AI and machine learning workloads has become mainstream, and Kubernetes is the de facto platform for orchestrating them. It’s a common assumption that managing a GPU in Kubernetes is like managing a CPU — it’s just a different kind of compute resource. Engineers often believe they can apply the same mental models of resource sharing, isolation, and monitoring they’ve honed over decades of working with CPUs and the Linux kernel.

This assumption is fundamentally wrong. The reality of how GPUs interact with the Linux kernel, the NVIDIA driver, and the Kubernetes control plane is filled with surprising, counter-intuitive behaviors that break every rule we thought we knew. Our trusted tools for resource management and isolation, built on kernel primitives like cgroups and namespaces, simply do not apply.

This article reveals seven of the most impactful and often misunderstood truths about running GPUs in Kubernetes at scale. Based on deep architectural analysis, these insights challenge the foundational assumptions many platform engineers hold, providing a clearer, more accurate model for building stable, efficient, and cost-effective GPU infrastructure.

### **1. The Hidden Truth: GPUs Have Always Supported Sharing, Just Not How You Think**

Contrary to the common belief that GPUs are monolithic devices that can’t be shared, they have always supported multiple processes running concurrently. However, this “sharing” operates on a principle completely different from the preemptive multitasking of a multi-core CPU. It’s not true parallel execution; it’s sequential turn-taking.

When multiple processes access the same GPU, the NVIDIA driver acts as a traffic controller. It queues the compute programs (**known as CUDA kernels**) from each process and executes them sequentially, one after another. This is a form of driver-level context switching.

The most critical concept to understand is that GPU kernels run to completion. They cannot be paused or preempted mid-execution. A kernel from Process A will monopolize the GPU’s compute units until it is finished. Only then can the driver schedule the next kernel from Process B’s queue. In effect, processes _take turns_ on the GPU. This distinction is vital because it breaks the mental model of true parallel multitasking that engineers are accustomed to with CPUs, forming the basis for many of the resource conflicts and performance mysteries that plague shared GPU environments.

### **2. The Great Misnomer: NVIDIA’s “Time-Slicing” Doesn’t Actually Slice Time**

The name of NVIDIA’s “time-slicing” feature is one of the most misleading in the cloud-native ecosystem. It creates false expectations of fairness, preemption, and guaranteed resource allocation that the technology simply does not deliver.

The feature does not actually manage or slice execution time. Instead, time-slicing is a configuration in the NVIDIA device plugin that effectively “lies” to Kubernetes. It makes a single physical GPU appear as multiple distinct, schedulable GPU resources. For example, you can configure one Tesla T4 to report `4` `nvidia.com/gpu` resources to the kubelet, allowing four different pods to be scheduled to that single physical card.

However, the feature does not enforce fair time shares among those pods, nor does it preempt long-running kernels to give other pods a turn. It has no awareness of memory usage and provides no mechanisms for memory enforcement. The underlying behavior remains the same: processes take turns, and one long-running kernel from a single pod will block all others.

What NVIDIA calls “time-slicing” should really be called “GPU multiplexing” or “replica mode.” It’s simply making one GPU appear as multiple GPUs to Kubernetes, with no actual time management involved.

### **3. The Root of All Problems: The Linux Kernel is Blind to GPUs**

In Kubernetes, all container isolation and resource management for CPUs and memory rely entirely on Linux kernel primitives like control groups (cgroups) and namespaces. Cgroups enforce hard limits on how much CPU time and memory a process can consume, while namespaces control what a process can see of the system. The kernel is the ultimate arbiter, with complete visibility and control.

These primitives do not exist for GPUs. The Linux kernel cannot see GPU memory allocations, it cannot track GPU compute utilization, and it cannot preempt a running GPU kernel. When a container makes a CUDA API call, the kernel sees a generic `ioctl()` call to the NVIDIA driver and nothing more. What happens next—the memory allocation, the kernel launch, the computation—is entirely opaque.

**The Linux kernel** is **utterly blind** to what happens on the GPU. This architectural blindness is the foundational reason why the entire Kubernetes multi-tenancy and resource control model completely collapses when applied to GPUs. Without kernel-level visibility and enforcement, concepts like fairness, quality of service, and hard resource limits become impossible to implement natively.

### **4. The 100% Utilization Lie: How a “Busy” GPU Can Be Doing Almost Nothing**

The `GPU-Util` percentage displayed by the `nvidia-smi` command is one of the most frequently misinterpreted metrics in infrastructure monitoring. It does not measure productivity, throughput, or how much work the GPU is actually accomplishing. This leads teams to believe their GPUs are maxed out when, in reality, they may be sitting mostly idle.

The metric works by sampling. The NVIDIA driver probes the GPU at set intervals (e.g., every 1/6th of a second) and asks a binary question: was _any_ kernel active on the GPU during this sampling window? If the answer is yes — even for a single microsecond — the entire window is marked as “active.” A GPU utilization of 100% simply means that in every sampling window, at least one kernel was running for some amount of time.

Consider a program that launches a series of tiny, trivial kernels. This workload can easily report 87–100% `GPU-Util` while the GPU's compute units are only productively working for 0.1% of the total time. The GPU is "busy" in the sense that it's constantly being touched by the driver, but it is not "productive." This lie has significant financial consequences, as organizations purchase more hardware, believing their current GPUs are at capacity when they are merely being used inefficiently.

### **5. The Undead Bug: Zombie Processes Can Hold GPU Memory Hostage**

One of the most insidious failure modes in a Kubernetes GPU cluster is the “zombie process” memory leak. This occurs when a pod using a GPU is terminated abruptly, such as by a `SIGKILL` signal from the kernel's Out-of-Memory (OOM) killer or a forced pod deletion.

Unlike CPU memory, which the kernel automatically reclaims upon a process’s death, a GPU application must make explicit cleanup calls to the NVIDIA driver to release its GPU memory. An abrupt termination prevents these calls from ever happening. The user-space process is gone, and the pod is deleted from Kubernetes, but the NVIDIA driver — which operates outside the kernel’s process management — is never notified.

The consequence is that the driver continues to hold the GPU memory reservation for a process that no longer exists. `nvidia-smi` will show no running processes, but a significant chunk of GPU memory remains allocated and unavailable. This creates a silent memory leak that starves future pods, causing them to fail with mysterious out-of-memory errors when they are scheduled to a GPU that appears to be free.

### **6. The Architect’s Dilemma: Why Seven Small GPUs Can Beat One Giant One**

When designing a platform for shared workloads, the intuitive choice might be to buy the biggest, most powerful GPU available, like an NVIDIA H100, and partition it using Multi-Instance GPU ([MIG](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/introduction.html)) technology. However, an architectural comparison reveals a surprising truth: for many use cases, a collection of cheaper, smaller GPUs can be a more resilient and economical choice.

Let’s compare an expensive H100 partitioned into seven MIG instances against seven individual, commodity Tesla T4 GPUs distributed across seven different nodes.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:831/1*2yD9cXpXoaYt2HxHommvbA.png)

While the H100 offers superior memory bandwidth and hardware-enforced isolation between its MIG instances, the distributed T4 model provides significant operational advantages. It is far more cost-efficient and offers true failure isolation — if one T4 node crashes, the other six workloads are completely unaffected. In contrast, if the single H100 node fails, all seven workloads go down. Furthermore, having seven distinct nodes provides far greater scheduling flexibility, allowing Kubernetes to spread workloads across different racks or even availability zones for enhanced resilience. For many shareable workloads that don’t require the H100’s peak performance, the distributed model is often the superior architectural choice.

### **7. The Magnificent Hack: Enforcing GPU Limits by Lying to Applications**

For GPUs that don’t support hardware partitioning like MIG, a powerful software enforcement model has emerged that can provide cgroup-like limits for memory. This approach, exemplified by projects like HAMi, works through a magnificent hack: API interception.

The core mechanism uses a standard Linux feature called `LD_PRELOAD`. This allows an administrator to inject a custom library into a process before any other libraries, including the official CUDA libraries, are loaded. This custom library acts as a proxy, intercepting every CUDA API call the application makes before it can reach the real NVIDIA driver.

This interception allows the library to enforce rules in software:

- **Memory Limits:** When an application calls `cuMemAlloc` to request GPU memory, the interception library can check if the request would exceed the pod's pre-configured limit. If so, it can deny the call and return an out-of-memory error to the application immediately, without ever involving the driver.
- **Lying to the App:** Well-behaved applications often query the GPU to see how much memory is available before allocating. When an application asks this question, the interception library can “lie.” Instead of reporting the physical GPU’s total memory (e.g., 16GB), it reports the pod’s configured limit (e.g., 4GB), effectively tricking the application into believing it’s running on a smaller GPU and staying within its assigned bounds.

## Conclusion: Rethinking Your GPU Strategy

The journey through the realities of GPU management in Kubernetes reveals a consistent theme: our mental models for resource management, built on decades of experience with CPUs and the Linux kernel, are not just unhelpful but actively misleading. The assumptions of preemptive multitasking, kernel-level enforcement, and trustworthy metrics all fall apart in the face of GPU architecture.

Understanding these surprising truths is the first step toward building truly efficient, stable, and cost-effective GPU platforms. Recognizing that the kernel is blind, that utilization metrics are deceptive, and that sharing is sequential turn-taking allows us to move beyond flawed assumptions. From hardware partitioning with MIG to clever software enforcement with API interception, the solutions require a new way of thinking — one that acknowledges the unique nature of the GPU.

Now that you know the kernel is blind and your metrics are lying, what’s the first assumption about your GPU cluster you’re going to challenge?