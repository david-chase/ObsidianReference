#ai #grove #howto #k8s #nvidia #orchestration #tutorial

Kubernetes changed how we run applications, but not how we runÂ **AI**.

It was designed forÂ **microservices**: small, independent apps that can scale up and down without much concern for each other.

**AI models are different**.

They are large, connected systems where several components must work together. One model might require multiple pods to handle different tasks simultaneously: some preparing the input, others generating the output, all exchanging data constantly.

Try running that with vanilla Kubernetes, and things start to break fast.

Thatâ€™s why NVIDIA createdÂ **Grove,**Â a new way toÂ **orchestrate**Â **complex AI systems**Â directly onÂ **Kubernetes**.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*wlnGxzMHbUVzI-r2nW8spQ.png)

## The Orchestration Gap Kubernetes Canâ€™t Solve Alone

Kubernetes excels at runningÂ **stateless**Â workloads, such as web APIs or backend services.  
But modern AI inference, which is the process of using a trained model to generate predictions or text, is anything but stateless.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*5QMmLVDA2EGWA0HumthPmg.png)

Hereâ€™s why:

- **Large models span multiple GPUs**  
    Large AI models are oftenÂ _sharded_, meaning theyâ€™re split across several nodes. A single model instance might involve five or ten pods working together. Scaling only one pod doesnâ€™t work when the real scaling unit is the entire group.
- **Startup ordering**  
    Certain components must start in a specific sequence. For example, worker pods need to be ready before the leader pod initializes.Â **Kubernetes doesnâ€™t enforce this logic natively**.
- **Gang scheduling**  
    Some pods must start together to be useful. Imagine launching a decode pod without a prefill pod, the system canâ€™t function. Without â€œall-or-nothingâ€ scheduling, GPUs sit idle waiting for missing components.
- **Topology awareness**  
    AI components exchange huge amounts of data. If those pods end up on nodes without high-speed connections (likeÂ [NVLink](https://www.nvidia.com/en-us/products/workstations/nvlink-bridges/)), performance collapses. Placement matters, a lot.

In short: Kubernetes knows how to scaleÂ _pods_.  
AI needs a way to scaleÂ **_systems_**_:_Â groups of pods that work as one.

## Meet NVIDIA Grove

Grove is aÂ **Kubernetes API**Â that finally makes the platform aware of AI.

It gives youÂ **one unified interface**Â to describe and run any inference workload â€” from a lightweight, single-pod model to a multi-node architecture spanning thousands of GPUs.

Instead of juggling multiple YAML files or custom controllers, you define your entire serving system, prefill, decode, routing, and every other component, inÂ **one declarative spec**.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*bC439zZbKIu-Fd6ifZDCBQ.png)

NVIDIA Grove

From that single definition, Grove handles all the orchestration behind the scenes: decidingÂ **when**Â each component starts,Â **where**Â it runs, andÂ **how**Â the system scales as demand grows.

Thatâ€™s possible because Grove understands the relationships inside your AI workload. It can:

- **Coordinate pods**Â that must launch together so the system never half-starts.
- Place components close to each other on the network toÂ **minimize**Â **latency**.
- Scale different roles independently to matchÂ **real-time demand**.
- **Control startup order**Â so everything initializes in the right sequence.

In short,Â **Grove**Â treats your AI model as one unified system, not a collection of pods.

## How Grove Works: Four Core Concepts

Grove introduces a few key primitives that describe how your model behaves and how its parts work together inside Kubernetes.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*t_TEHBUSS16V-WXbE7QxEQ.png)

**ğŸ”¹ï¸ PodCliqueSet:**Â At the top of the hierarchy, the PodCliqueSet defines the complete inference system. It governs how components scale, start, and are placed across nodes, ensuring the system behaves as one coordinated workload.

**ğŸ”¹ PodClique:**Â A group of pods that share the same role, such as a set of decode workers or a frontend router. This allows each role in your architecture to be configured, managed, and scaled independently.

**ğŸ”¹ PodCliqueScalingGroup:**Â A collection of PodCliques that need to run and scale together. It keeps interdependent components, like prefill and decode, synchronized so the entire system starts and operates cohesively.

**ğŸ”¹ï¸ PodGang:**Â TheÂ **link**Â between Grove and the Kubernetes scheduler. It ensures that groups of related pods are deployed together as a single unit, waiting if necessary until all required resources are available.

## Quick Start: Try Grove in 5 Minutes

If your Kubernetes cluster is already set up (andÂ `kubectl`Â is configured), you can get Grove running in just a few commands.

### 1. InstallÂ **Grove**

git clone https://github.com/NVIDIA/grove.git  
cd grove  
make deploy  
  
kubectl get pods -n grove-system

This installs the GroveÂ **operator**,Â **CRDs**, and supporting components.

### 2. Deploy a Sample Workload

kubectl apply -f samples/simple/simple1.yaml

This launches aÂ **minimal example**Â showing Groveâ€™s orchestration in action.

### 3. Inspect Grove Resources

kubectl get pcs,pclq,pcsg,pg,pod -o wide

Youâ€™ll see Grove-created objects likeÂ `PodClique`,Â `PodCliqueSet`, andÂ `PodGang`, which coordinate your workload as a single logical unit.

For cloud and production deployments, NVIDIA provides full installation and configuration guides in theÂ [official docs](https://github.com/NVIDIA).

## Example Use Cases

Grove simplifies AI workload orchestration across any setup:

- **Multi-GPU inference:**Â Run large models like DeepSeek-R1 or Llama-4 with coordinated scaling and placement.
- **Real-life chatbot:**Â A multilingual bot (router + prefill + decode pods) defined in one YAML. Grove spawns all components together, keeps them synced, and scales each role independently.
- **Agentic pipelines:**Â Orchestrate multiple collaborating models with correct startup and topology awareness.
- **Single-GPU tasks:**Â Even simple models get consistent, automated orchestration.

In short, Grove turns complex, multi-component AI deployments into a single, declarative workflow.