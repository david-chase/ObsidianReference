#cilium #k8s #networking #medium #cni 

When looking at how Kubernetes handles internal communication, it helps to start with the basics. Inside a Kubernetes cluster, Pods can talk to each other whether they run on the same node or across different nodes. This is possible because every Pod gets its own IP address, and the cluster’s CNI (Container Network Interface) handles the routing behind the scenes. In this case, the CNI is **Cilium**.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*e2ZJFC9NMhC5prf67xvvgA.png)

When Pods run on the same node, communication stays local and travels over a virtual Ethernet pair. When Pods run on different nodes, the CNI routes that traffic across the cluster network. Different CNIs use different techniques to make this work, but the goal is always the same: provide a **flat, direct Pod-to-Pod networking model** without requiring NAT.

Cilium goes further by using **eBPF inside the Linux kernel** instead of traditional overlays. This provides faster data processing, more efficient routing, and deeper visibility into the traffic flowing through the cluster.

Each Kubernetes node runs a Cilium agent that loads and manages the eBPF programs responsible for routing, filtering, and processing packets. By default, Pods can reach each other inside the cluster, but external traffic cannot reach them unless configured.

Kubernetes also offers built-in ways to shape and control communication:

- **NetworkPolicies** manage which Pods are allowed to talk to each other.
- **Services** provide stable DNS names so workloads can reliably communicate even as Pods restart or move between nodes.

All of this forms the foundation of Kubernetes networking. However, the moment a cluster grows, it becomes hard to know **which Pods are actually communicating**, and even harder to enforce the right security rules. This is where **Cilium** and **Hubble** become essential tools.

## Why Cilium Makes Things Easier

Once the basics of Kubernetes networking are clear, the real challenge becomes managing it at scale. Traditional CNIs rely heavily on IP-based rules in iptables. As Pods restart and IPs change, these rules become difficult to maintain and provide almost no visibility.

Cilium solves this by:

- Using **eBPF** to handle traffic directly inside the kernel
- Making decisions based on **Pod labels**, not Pod IPs
- Maintaining **stable and predictable network policies** even during Pod churn
- Providing full visibility using **Hubble**, which shows real-time network flows

Hubble displays which Pods are talking, on which ports, and whether the traffic is allowed or denied. This level of detail is something most CNIs cannot offer.

## When This Approach Helps

Cilium and Hubble become extremely helpful when you need:

- To validate that no unexpected or unnecessary Pod-to-Pod communication exists
- To restrict communication to specific services or workloads
- To control outbound traffic to only a trusted set of external domains

This combination makes the cluster more predictable, more secure, and easier to troubleshoot as it grows.

## 1. Install Cilium CLI and Deploy Cilium

Before installing Cilium, make sure you have a working Kubernetes cluster and that `kubectl` is configured.  
Then run:

curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}  
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum  
sudo tar -C /usr/local/bin -xzvf cilium-linux-amd64.tar.gz  
rm cilium-linux-amd64.tar.gz{,.sha256sum}  
cilium install --set hubble.relay.enabled=true --set hubble.ui.enabled=true  
cilium status --wait

## 2. Visualize Pod Traffic with Hubble

Once Cilium is installed, [Hubble](https://docs.cilium.io/en/stable/observability/hubble/hubble-ui/) immediately begins capturing flows.  
Open the UI:

cilium hubble ui

The service map shows Pod-to-Pod communication as simple visual links, making it easy to spot patterns and identify unexpected traffic.

## 3. Control Traffic with a CiliumNetworkPolicy

After understanding the traffic, you can start applying precise rules.  
This example allows only Pods labeled `app=payments` to reach Pods labeled `app=backend` on port 8090.

Create `allow-policy.yaml`:

apiVersion: "cilium.io/v2"  
kind: CiliumNetworkPolicy  
metadata:  
  name: "allow-frontend-to-backend"  
  namespace: default  
spec:  
  endpointSelector:  
    matchLabels:  
      app: backend  
  ingress:  
    - fromEndpoints:  
      - matchLabels:  
          app: payments  
      toPorts:  
        - ports:  
          - port: "8090"  
            protocol: TCP

Apply it:

kubectl apply -f allow-policy.yaml

Refresh Hubble and any denied traffic will appear clearly, so you instantly see whether the rule is working.

## Understanding How Cilium Makes Traffic Decisions

Cilium evaluates each packet in a predictable order:

1. A packet enters or leaves a Pod
2. Cilium reads the labels on both Pods
3. It checks for matching policies in that namespace
4. If a rule explicitly allows it, the packet is accepted
5. If no rule matches, the default behavior applies (allow unless restricted)

This explains why new clusters initially feel “open.”  
Once you begin adding policies, Cilium enforces them exactly and consistently.

## Helpful Tips for Using Cilium and Hubble

### Tip 1: Always label your Pods clearly

Cilium policies depend heavily on labels. Good labeling dramatically improves policy clarity.

### Tip 2: Start with visualization before applying policies

Use Hubble’s service map to understand actual traffic before writing policies.

### Tip 3: Test policies in isolated namespaces

It reduces accidental broad denials during early experimentation.

### Tip 4: Use DNS-based egress policies

This keeps configurations stable even when external IPs change.

### Tip 5: Know that Cilium is service-mesh-ready

You can enable L7 visibility without using a full service mesh.

## Useful Cilium and Hubble Commands

Below are some practical commands that help with debugging, inspecting flows, and validating policies in a Cilium-powered cluster.

### Check Cilium Status

cilium status

Shows whether the Cilium agents, operator, and Hubble components are working correctly.

### Verify Connectivity Between Pods

cilium connectivity test

Runs a full set of connectivity checks across the cluster, including DNS, L3/L4/L7 policies, and service routing.

### View Recent Network Flows Using Hubble CLI

hubble observe

Streams real-time traffic flows directly to your terminal.

### Filter by Namespace or Labels

hubble observe --namespace default  
hubble observe --label app=backend

Filters traffic so you see only what is relevant.

### Show Flow Summary

hubble status

Displays the status of the Hubble components and verifies that visibility is enabled.

## Summary

- **Cilium** uses eBPF to manage Pod communication based on labels, not IPs
- **Hubble** provides real-time visibility into Pod flows and network decisions
- **CiliumNetworkPolicy** lets you restrict or authorize traffic with precision
- Cilium evaluates traffic through labels and matching rules, keeping behavior predictable and consistent