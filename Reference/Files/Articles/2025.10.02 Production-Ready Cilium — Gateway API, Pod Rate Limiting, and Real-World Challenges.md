#cilium #cni #medium #networking 

## **Battle-Tested in Production: Advanced Cilium Features and Real-World Deployment Strategies**

## Introduction: From Lab to Production

In Parts 1‚Äì3, we explored Cilium‚Äôs origins, technical architecture, and observability capabilities. We‚Äôve seen the theory, the architecture diagrams, and the lab examples. Now it‚Äôs time to address the question every platform engineer asks:¬†**‚ÄúHow do I actually run this in production?‚Äù**

Production Kubernetes deployments face challenges that don‚Äôt appear in demos:

- **Traffic surges**¬†that can overwhelm pods
- **Certificate management**¬†for secure ingress
- **Multi-cluster coordination**¬†across clouds
- **Monitoring at scale**¬†with thousands of services
- **Compliance requirements**¬†demanding audit trails
- **Cost optimization**¬†without sacrificing performance

This final article focuses on production-grade features that separate successful deployments from troubled ones. We‚Äôll cover:

**Gateway API**: The next-generation ingress with TLS termination, traffic splitting, and multi-tenant separation ‚Äî all without sidecars.

**Pod Rate Limiting**: Protecting services from noisy neighbors and traffic spikes using eBPF bandwidth management.

**Grafana Integration**: Production monitoring with Prometheus metrics, alerting, and golden signals dashboards.

**Real-World Challenges**: Battle-tested strategies from companies running Cilium at scale ‚Äî what works, what doesn‚Äôt, and how to avoid common pitfalls.

**What makes this different from Parts 1‚Äì3**: This isn‚Äôt about how Cilium works internally ‚Äî it‚Äôs about how to operate it successfully. You‚Äôll learn from actual production deployments at companies like Form3, IKEA, Palantir, and others who‚Äôve navigated the journey from proof-of-concept to mission-critical infrastructure.

**üîó GitHub Repository**: All manifests, Grafana dashboards, and troubleshooting guides:  
[**cilium-deep-dive-labs**](https://github.com/Salwan-Mohamed/cilium-deep-dive-labs)¬†‚Äî See¬†`labs/05-production-features/`¬†for complete examples.

Let‚Äôs dive into production-ready Cilium.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:831/0*KOUBNheeOnVk_949)

## Chapter 1: Gateway API ‚Äî Modern Ingress for Production

## Why Gateway API Matters

Traditional Kubernetes Ingress has served us well, but it has limitations:

- **Single-layer configuration**: Can‚Äôt separate platform and application concerns
- **Limited features**: No native traffic splitting, header manipulation, or advanced routing
- **Vendor extensions**: Each Ingress controller has custom annotations
- **TLS complexity**: Certificate management is cumbersome

**Gateway API**¬†solves these problems with a role-oriented design:

**Platform Team**¬†manages:

- `GatewayClass`: Infrastructure provider (Cilium)
- `Gateway`: Listeners, ports, TLS certificates

**Application Team**¬†manages:

- `HTTPRoute`: Routing rules, backends
- `GRPCRoute`: gRPC-specific routing

This separation enables¬†**self-service**¬†while maintaining¬†**control**.

## TLS Termination with Gateway API

Let‚Äôs walk through a production-grade HTTPS ingress setup.

**Step 1: Create TLS Certificate**

# Generate certificate for your domain  
mkcert bookinfo.cilium.rocks

# Output: bookinfo.cilium.rocks.pem and bookinfo.cilium.rocks-key.pem# Store as Kubernetes secret  
kubectl create secret tls bookinfo-tls \  
  --cert=bookinfo.cilium.rocks.pem \  
  --key=bookinfo.cilium.rocks-key.pem \  
  --namespace=default

**Step 2: Deploy Gateway**

apiVersion: gateway.networking.k8s.io/v1beta1  
kind: Gateway  
metadata:  
  name: https-gateway  
  namespace: default  
spec:  
  gatewayClassName: cilium  
  listeners:  
  - name: https  
    protocol: HTTPS  
    port: 443  
    hostname: "bookinfo.cilium.rocks"  
    tls:  
      mode: Terminate  
      certificateRefs:  
      - kind: Secret  
        name: bookinfo-tls

**What happens**:

1. Cilium creates a LoadBalancer service
2. External IP is automatically allocated (via MetalLB or cloud provider)
3. TLS certificates loaded from secret
4. Ready to receive HTTPS traffic

**Step 3: Define HTTPRoute**

apiVersion: gateway.networking.k8s.io/v1beta1  
kind: HTTPRoute  
metadata:  
  name: bookinfo-route  
  namespace: default  
spec:  
  parentRefs:  
  - name: https-gateway  
  hostnames:  
  - "bookinfo.cilium.rocks"  
  rules:  
  - matches:  
    - path:  
        type: PathPrefix  
        value: /details  
    backendRefs:  
    - name: details  
      port: 9080  
  - matches:  
    - path:  
        type: PathPrefix  
        value: /reviews  
    backendRefs:  
    - name: reviews  
      port: 9080

**Routing logic**:

- `https://bookinfo.cilium.rocks/details`¬†‚Üí details service
- `https://bookinfo.cilium.rocks/reviews`¬†‚Üí reviews service
- Automatic TLS termination at gateway

**Step 4: Test**

# Get gateway IP  
GATEWAY_IP=$(kubectl get svc -l "gateway.networking.k8s.io/gateway-name=https-gateway" \  
  -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}')  
# Update /etc/hosts  
echo "$GATEWAY_IP bookinfo.cilium.rocks" >> /etc/hosts  
# Test HTTPS connection  
curl https://bookinfo.cilium.rocks/details  
# Returns: Book details in JSON

**Certificate verified, TLS terminated, traffic routed**¬†‚Äî all with zero application changes.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:831/0*ftdHkrYAirvRabu4)

Advanced Gateway API Features

**Header-Based Routing**:

rules:  
- matches:  
  - headers:  
    - name: "X-User-Type"  
      value: "premium"  
  backendRefs:  
  - name: premium-backend  
- matches:  
  - headers:  
    - name: "X-User-Type"  
      value: "standard"  
  backendRefs:  
  - name: standard-backend

**Traffic Splitting**¬†(Canary/Blue-Green):

rules:  
- backendRefs:  
  - name: app-v1  
    weight: 90  
  - name: app-v2  
    weight: 10  # 10% to new version

**Query Parameter Routing**:

rules:  
- matches:  
  - queryParams:  
    - name: "version"  
      value: "beta"  
  backendRefs:  
  - name: beta-backend

## Production Benefits

**IKEA Use Case**: Replaced external F5 load balancers with Cilium Gateway API:

- **Cost savings**: Eliminated expensive hardware
- **Cloud-native management**: GitOps with Flux
- **Faster deployments**: No ticket to network team
- **Better observability**: Hubble integration

**üîó Repository**: Complete Gateway API examples in¬†`labs/05-production-features/gateway-api/`

## Chapter 2: Pod Rate Limiting ‚Äî Traffic Management at Scale

## The Noisy Neighbor Problem

In multi-tenant Kubernetes clusters, one misbehaving pod can saturate network bandwidth, starving other workloads. Traditional solutions (QoS, network plugins) are complex and often ineffective.

**Cilium‚Äôs solution**: eBPF-based¬†**bandwidth management**¬†that enforces per-pod rate limits at the kernel level with negligible overhead.

## Enabling Bandwidth Manager

Bandwidth manager must be explicitly enabled:

# Via Helm  
helm upgrade cilium cilium/cilium \  
  --namespace kube-system \  
  --set bandwidthManager.enabled=true \  
  --reuse-values  
# Restart Cilium agents  
kubectl -n kube-system rollout restart ds/cilium

Verify it‚Äôs enabled:

kubectl -n kube-system exec ds/cilium -- cilium status | grep -i bandwidth  
# Should show: BandwidthManager: true

## Configuring Per-Pod Limits

Rate limits are set via¬†**pod annotations**:

apiVersion: v1  
kind: Pod  
metadata:  
  name: netperf-server  
  annotations:  
    # Egress bandwidth limit  
    kubernetes.io/egress-bandwidth: "10M"  # 10 Mbps  
    # Ingress bandwidth limit (optional)  
    kubernetes.io/ingress-bandwidth: "100M"  # 100 Mbps  
spec:  
  containers:  
  - name: netperf  
    image: networkstatic/netperf

**Supported units**:¬†`K`¬†(Kbps),¬†`M`¬†(Mbps),¬†`G`¬†(Gbps)

## Testing Rate Limits

**Deploy server with 10Mbps limit**:

apiVersion: apps/v1  
kind: Deployment  
metadata:  
  name: netperf-server  
spec:  
  replicas: 1  
  selector:  
    matchLabels:  
      app: netperf-server  
  template:  
    metadata:  
      labels:  
        app: netperf-server  
      annotations:  
        kubernetes.io/egress-bandwidth: "10M"  
    spec:  
      containers:  
      - name: netperf  
        image: networkstatic/netperf  
        command: ["netserver"]  
        ports:  
        - containerPort: 12865

**Deploy client on different node**:

apiVersion: v1  
kind: Pod  
metadata:  
  name: netperf-client  
spec:  
  affinity:  
    podAntiAffinity:  
      requiredDuringSchedulingIgnoredDuringExecution:  
      - labelSelector:  
          matchLabels:  
            app: netperf-server  
        topologyKey: kubernetes.io/hostname  
  containers:  
  - name: netperf  
    image: networkstatic/netperf

**Run performance test**:

# Get server IP  
SERVER_IP=$(kubectl get pod -l app=netperf-server -o jsonpath='{.items[0].status.podIP}')  
# Execute test from client  
kubectl exec netperf-client -- netperf -H $SERVER_IP -l 10  
# Result: 9.53 Mbps (just under 10Mbps limit!)

## Verifying eBPF Enforcement

# Find Cilium pod on same node as server  
CILIUM_POD=$(kubectl -n kube-system get pod -l k8s-app=cilium \  
  --field-selector spec.nodeName=$(kubectl get pod netperf-server-xxx -o jsonpath='{.spec.nodeName}') \  
  -o jsonpath='{.items[0].metadata.name}')  
# Check eBPF rate limit configuration  
kubectl -n kube-system exec $CILIUM_POD -- \  
  cilium bpf bandwidth list  
# Output shows:  
# IDENTITY    EGRESS BANDWIDTH  
# 3825        10 Mbit

**The limit is enforced in eBPF**¬†‚Äî kernel-level, per-identity, with minimal CPU overhead.

## Updating Limits

# Change limit to 100Mbps  
kubectl patch deployment netperf-server -p \  
  '{"spec":{"template":{"metadata":{"annotations":{"kubernetes.io/egress-bandwidth":"100M"}}}}}'  
# Pods restart with new limit  
# Re-run test  
kubectl exec netperf-client -- netperf -H $SERVER_IP -l 10  
# Result: 95.4 Mbps (just under 100Mbps!)

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:831/0*ZD4jErvg6d34Yr1w)

## Production Use Cases

**Multi-Tenant SaaS**: Prevent one customer‚Äôs workload from starving others¬†**Data Processing**: Limit batch job bandwidth to protect real-time services¬†**Cost Control**: Prevent runaway data transfer costs in cloud environments

**Best Practices**:

- Set limits based on P95 observed usage, not peak
- Monitor bandwidth utilization with Hubble metrics
- Use namespace-level defaults for consistency
- Test limits in staging before production

**üîó Repository**: Rate limiting examples and testing tools in¬†`labs/05-production-features/rate-limiting/`

## Chapter 3: Grafana Monitoring ‚Äî Production Observability

## The Monitoring Stack

Production Cilium deployments require comprehensive monitoring:

- **Cilium agent health**: CPU, memory, eBPF program status
- **Network flows**: Traffic patterns, drops, errors
- **Policy enforcement**: Allow/deny decisions
- **Service performance**: Latency, throughput, errors

**The stack**: Cilium ‚Üí Prometheus ‚Üí Grafana

## Setting Up Prometheus and Grafana

**Step 1: Deploy Monitoring Stack**

# Add Prometheus/Grafana via manifests  
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.15.0/examples/kubernetes/addons/prometheus/monitoring-example.yaml  
# Wait for pods  
kubectl -n cilium-monitoring get pods  
# Port forward Grafana  
kubectl -n cilium-monitoring port-forward svc/grafana 3000:3000

**Step 2: Configure Cilium Metrics**

Ensure Cilium exports metrics:

# Cilium Helm values  
hubble:  
  metrics:  
    enabled:  
    - dns  
    - drop  
    - tcp  
    - flow  
    - icmp  
    - http  
    serviceMonitor:  
      enabled: true  
prometheus:  
  enabled: true  
  serviceMonitor:  
    enabled: true  
operator:  
  prometheus:  
    enabled: true  
    serviceMonitor:  
      enabled: true

**Step 3: Access Grafana**

1. Navigate to¬†`[http://localhost:3000](http://localhost:3000/)`
2. Login: admin/admin
3. Pre-configured dashboards available:

- **Cilium Metrics**: Agent health, resource usage
- **Cilium Operator**: Control plane metrics
- **Hubble**: Network flows and golden signals

## Key Dashboards Explained

**Cilium Metrics Dashboard**:

**Agent Health Panel**:

- CPU usage per node
- Memory consumption
- eBPF map pressure
- API rate limiting

**Example alert**: High memory usage

- alert: CiliumHighMemory  
  expr: container_memory_usage_bytes{pod=~"cilium-.*"} / container_spec_memory_limit_bytes > 0.8  
  for: 5m  
  annotations:  
    summary: "Cilium agent using >80% memory"

**BPF Map Pressure**: Critical metric ‚Äî full maps cause packet drops

cilium_bpf_map_pressure{map_name="cilium_policy_00001"}

**Network Flow Metrics**:

**Flow Distribution**:

sum(rate(hubble_flows_processed_total[5m])) by (destination_namespace)

Shows which namespaces receive most traffic.

**DNS Query Patterns**:

rate(hubble_dns_queries_total[5m])

Spike in DNS queries may indicate:

- New service discovery
- DNS issues (retry loops)
- Potential DNS amplification attack

**HTTP Golden Signals**:

**Request Rate**:

sum(rate(hubble_http_requests_total[5m])) by (destination_workload)

**Error Rate**:

sum(rate(hubble_http_requests_total{code=~"5.."}[5m])) /   
sum(rate(hubble_http_requests_total[5m]))

**Latency (P95)**:

histogram_quantile(0.95,   
  rate(hubble_http_request_duration_seconds_bucket[5m])  
)

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:831/0*3VhMPHXM79qX_II8)

## Real-World Monitoring Scenarios

**Scenario 1: Detecting Policy Misconfigurations**

**Symptom**: Application reports intermittent connectivity issues.

**Investigation**:

1. Check Hubble drops dashboard
2. Filter by source application
3. Observe pattern: drops only to specific backend

**Root cause**: Network policy missing port 8080, only allowing 80.

**Resolution**: Update policy, verify drops cease.

**Scenario 2: Capacity Planning**

**Question**: Do we need more nodes?

**Metrics to check**:

- Cilium agent CPU/memory trends
- eBPF map utilization
- Flow processing rate
- Connection tracking entries

**Decision**: If map pressure consistently >80%, scale cluster.

## Alerting Best Practices

**Critical Alerts**¬†(page on-call):

- Cilium agent down
- eBPF map full (causes drops)
- High policy drop rate (>1000/min)

**Warning Alerts**¬†(ticket):

- High memory usage (>80%)
- Elevated DNS query failures
- Abnormal flow patterns

**Informational**:

- New pod churn rate
- Certificate expiration warnings

**üîó Repository**: Complete Grafana dashboards and alert rules in¬†`labs/05-production-features/grafana-monitoring/`

## Chapter 4: Real-World Production Challenges

## Challenge 1: Onboarding Applications Securely

**The Problem**: How do you deploy apps in a zero-trust environment without breaking everything?

**Company**: Palantir (Government/Defense contractor)

**Challenge**:

- Strict security requirements: default-deny all traffic
- Hundreds of microservices with complex dependencies
- Application teams need autonomy
- Security team needs compliance proof

**Solution**: Phased onboarding with Hubble-driven policy generation

**Phase 1**: Observation Mode

# Deploy in permissive namespace  
apiVersion: v1  
kind: Namespace  
metadata:  
  name: app-onboarding  
  annotations:  
    policy-mode: "permissive"  # Allow all, observe

**Phase 2**: Generate Traffic and Observe

# Run integration tests  
./run-app-tests.sh  
# Observe flows for 24 hours  
hubble observe --namespace app-onboarding --since 24h | \  
  hubble-policy-generator > recommended-policies.yaml

**Phase 3**: Apply and Test

# Apply generated policies  
kubectl apply -f recommended-policies.yaml  
# Monitor for unexpected drops  
hubble observe --namespace app-onboarding --verdict DROPPED --follow

**Phase 4**: Iterate and Harden

- Fix legitimate drops by updating policies
- Remove unused allow rules
- Add audit logging for sensitive endpoints

**Results**:

- Onboarding time: Weeks ‚Üí Days
- Policy coverage: 100%
- False positives: <5%
- Developer satisfaction: High (self-service with guardrails)

**Key Lesson**:¬†**Start permissive, observe, then restrict**. Don‚Äôt go zero-trust day one.

## Challenge 2: Multi-Cloud Networking

**The Problem**: How do you connect workloads across AWS, GCP, and on-prem?

**Company**: Form3 (FinTech, multi-cloud payment platform)

**Challenge**:

- Services in AWS, GCP, on-prem data centers
- Regulatory requirements: data sovereignty
- Need unified security policies
- Must support failover across clouds

**Solution**: Cluster Mesh with encryption

**Architecture**:

AWS Cluster 1 (us-east-1)  
  ‚Üï encrypted tunnel  
On-Prem Cluster (EU data center)  
  ‚Üï encrypted tunnel  
GCP Cluster (europe-west1)

**Implementation**:

# Enable Cluster Mesh  
cilium clustermesh enable --context cluster1  
cilium clustermesh enable --context cluster2  
# Connect clusters  
cilium clustermesh connect \  
  --context cluster1 \  
  --destination-context cluster2  
# Enable WireGuard encryption  
helm upgrade cilium cilium/cilium \  
  --set encryption.enabled=true \  
  --set encryption.type=wireguard

**Cross-Cluster Services**:

apiVersion: v1  
kind: Service  
metadata:  
  name: payment-api  
  annotations:  
    service.cilium.io/global: "true"  # Available in all clusters  
    service.cilium.io/shared: "true"  
spec:  
  type: ClusterIP  
  ports:  
  - port: 8080

**Network Policies Across Clusters**:

apiVersion: cilium.io/v2  
kind: CiliumNetworkPolicy  
metadata:  
  name: allow-payment-api  
spec:  
  endpointSelector:  
    matchLabels:  
      app: payment-processor  
  ingress:  
  - fromEndpoints:  
    - matchLabels:  
        app: web-frontend  
        io.cilium.k8s.policy.cluster: cluster1  # Specific cluster  
    - matchLabels:  
        app: web-frontend  
        io.cilium.k8s.policy.cluster: cluster2

**Results**:

- Unified networking across 3 environments
- <5ms latency between clusters (same region)
- Automatic failover during AWS outage
- Single pane of glass for observability

**Key Lesson**:¬†**Cluster Mesh isn‚Äôt magic**¬†‚Äî you still need proper routing between clouds (VPN, Direct Connect). Cilium handles the rest.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:831/0*bF3_5vL2n1fN8w_Q)

## Challenge 3: Large-Scale Performance

**The Problem**: Does Cilium actually scale to thousands of nodes?

**Company**: Datadog (Monitoring platform, massive scale)

**Challenge**:

- 1000+ node clusters
- 10,000+ services
- Millions of network flows per second
- Cilium itself must be monitorable

**Bottlenecks Encountered**:

**1. eBPF Map Limits**: Early on, hit default map size limits causing drops.

**Solution**: Increased map sizes

hubble:  
  eventBufferSize: 65535  # Default: 4096  
    
bpf:  
  ctMax: 524288  # Connection tracking entries  
  natMax: 524288  # NAT entries

**2. Hubble Relay Overload**: Central relay couldn‚Äôt keep up with flow volume from all nodes.

**Solution**: Sampling + local caching

hubble:  
  relay:  
    replicas: 3  # Scale horizontally  
  metrics:  
    sampling:  
      enabled: true  
      rate: 10  # Sample 1 in 10 flows

**3. Policy Update Latency**: With 10K services, policy updates took 30+ seconds.

**Solution**: Incremental policy updates (Cilium 1.12+)

- Only affected endpoints recalculate policies
- Reduced update time to <5 seconds

**Results**:

- Stable at 1200+ nodes
- <1% CPU overhead per node
- 50M+ flows/day observable
- Policy updates: <5s even at scale

**Key Lesson**:¬†**Cilium scales, but you must tune it**. Default settings are for 50‚Äì100 node clusters.

## Challenge 4: Compliance and Audit

**The Problem**: How do you prove network security to auditors?

**Company**: Financial Services (Anonymous, SOC2 + PCI-DSS)

**Requirements**:

- 7-year audit trail of all network flows
- Proof of policy enforcement
- Ability to query historical data
- Tamper-proof logs

**Solution**: Hubble + Timescape + S3

**Architecture**:

Hubble Flows ‚Üí Timescape (ClickHouse) ‚Üí Daily S3 Export ‚Üí Glacier  
              ‚Üì  
         Grafana (Ops)  
              ‚Üì  
     Compliance Dashboard

**Daily Export Script**:

#!/bin/bash  
DATE=$(date +%Y-%m-%d)  
BUCKET="s3://compliance-audit-logs"  
# Export previous day's flows  
hubble observe --since 24h --output jsonpb | \  
  gzip > flows-$DATE.json.gz  
# Upload to S3 with encryption  
aws s3 cp flows-$DATE.json.gz \  
  $BUCKET/hubble-flows/$DATE/ \  
  --server-side-encryption AES25  
# Verify upload  
aws s3 ls $BUCKET/hubble-flows/$DATE/

**Compliance Queries**¬†(via Timescape):

-- Show all denied connections to production  
SELECT   
  timestamp,  
  source_namespace,  
  source_pod,  
  destination_namespace,  
  destination_pod,  
  verdict_reason  
FROM hubble_flows  
WHERE   
  destination_namespace = 'production'  
  AND verdict = 'DROPPED'  
  AND timestamp > now() - INTERVAL 30 DAY  
ORDER BY timestamp DESC;

**Audit Report Generation**:

- Automated monthly reports
- Policy coverage metrics
- Anomaly detection
- Executive summary

**Results**:

- Passed SOC2 and PCI-DSS audits
- Reduced audit prep time: Weeks ‚Üí Hours
- Complete 7-year trail in S3
- Cost: ~$500/month for storage

**Key Lesson**:¬†**Compliance is built-in with Hubble**¬†‚Äî you just need to export and retain the data.

## Chapter 5: Production Best Practices

## Deployment Patterns

**Pattern 1: Canary Roll-out**

Don‚Äôt deploy Cilium cluster-wide immediately. Start small:

1. **Pilot cluster**¬†(non-production)

- Test all features
- Validate policies
- Train team

**2. Dev/staging**¬†clusters

- Real applications, safe environment
- Performance testing
- Catch integration issues

**3. Production ‚Äî one namespace**

- Low-risk application
- Monitor closely for 1 week

**4. Production ‚Äî gradual expansion**

- Namespace by namespace
- Critical apps last

**Pattern 2: Multi-Cluster Strategy**

**For large orgs**:

- **Cluster per environment**: dev, staging, prod
- **Cluster per region**: Reduce cross-region latency
- **Cluster per team**: Isolation for security/compliance
- **Cluster Mesh**: Connect them all

**Pattern 3: Hybrid CNI**

Some orgs run¬†**Cilium alongside existing CNI**:

- Legacy CNI for ‚Äúboring‚Äù workloads
- Cilium for applications needing advanced features

Not ideal long-term, but helps migration.

## Operational Runbooks

**Incident: Cilium Agent Crash Loop**

**Symptoms**: Pods stuck in ContainerCreating

**Investigation**:

# Check agent logs  
kubectl -n kube-system logs -l k8s-app=cilium --tail=100  
# Common causes:  
# - eBPF program compilation failure  
# - Kernel version incompatibility  
# - Resource exhaustion

**Resolution**:

- Check kernel version compatibility
- Increase agent resource limits
- Review recent config changes

**Incident: Network Policy Not Working**

**Symptoms**: Traffic allowed that should be denied

**Investigation**:

# Check policy applied  
kubectl describe ciliumnetworkpolicy <name>  
# Check endpoint has policy  
kubectl get cep -n <namespace> <pod> -o yaml | grep policy  
# Test policy decision  
cilium policy trace --src-pod <source> --dst-pod <dest>

**Resolution**:

- Verify label selectors match pods
- Check for conflicting policies (deny vs allow)
- Restart Cilium agent if policy not applied

## Upgrades and Maintenance (continued)

**Upgrade Strategy**:

1. **Test in lab**¬†with same workloads
2. **Backup current config**

helm get values cilium -n kube-system > cilium-backup.yaml

**3.Review release notes**¬†for breaking changes

**4. Upgrade non-production first**

helm upgrade cilium cilium/cilium \  --version 1.15.0 \  --namespace kube-system \  --reuse-values

**5. Monitor for issues**¬†(24‚Äì48 hours)

**6. Production upgrade**¬†during maintenance window

**Rollback Plan**:

# If issues occur  
helm rollback cilium -n kube-system  
kubectl -n kube-system rollout restart ds/cilium