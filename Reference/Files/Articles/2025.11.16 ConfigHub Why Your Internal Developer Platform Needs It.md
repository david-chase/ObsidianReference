#medium #confighub #configuration #cad #canonical 

![](https://miro.medium.com/v2/resize:fit:1050/1*O86QD0wAwDZnICCRI5DemA.png)

**Fig. 0:** ConfigHub — A new paradigm shift in the GitOps space?

This is not a [GitOps](https://about.gitlab.com/topics/gitops/) problem.  
It is a problem we created through GitOps, for GitOps.

We are now in the middle of a **paradigm shift** in how we think about configuration and how internal developer platforms evolve.  
It is time to see configuration as a first-class part of platform design, not just something stored in Git.

There is an unspoken challenge in the market when it comes to GitOps.

The original idea [behind GitOps was simple](https://akuity.io/blog/getting-into-gitops):

- Declarative
- Versioned and Immutable
- Pulled Automatically
- Continuously Reconciled

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*19TbHgqkpxioe34ag288Hw.png)

**Fig.1:** [The Four Key Principles of GitOps](https://akuity.io/blog/getting-into-gitops)

Let’s take a closer look at what we have made of it today.  
Even the founder of GitOps now talks about [_config sprawl_](https://techcrunch.com/2025/03/26/cloud-veterans-launch-confighub-to-fix-configuration-hell/) — configs scattered everywhere.

## Challenge with Helm

In most cases, we use [Helm](https://helm.sh/docs/intro/using_helm/) or Helm [Umbrella Charts](https://helm.sh/docs/howto/charts_tips_and_tricks/) to deploy third-party tools, also called addons, like [Cert-Manager](https://cert-manager.io/), [External-Secrets-Operator](https://external-secrets.io/latest/), [Kyverno](https://kyverno.io/), [External-DNS](https://github.com/kubernetes-sigs/external-dns), and others.

We do something like this:

apiVersion: v2  
name: cert-manager  
description: Umbrella Chart for cert-manager  
type: application  
version: 0.0.2  
dependencies:  
  - name: cert-manager  
    version: v1.18.2  
    repository: https://charts.jetstack.io

We create umbrella Helm charts, overwrite the values we need, and add missing resources like:

.  
├── Chart.yaml  
├── templates  
│   ├── clusterissuer.yaml  
│   ├── namespace.yaml  
│   └── prometheusRule.yaml  
└── values.yaml

and it looks like this:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*dPcvVwh6vUz_K1EOa-kORg.png)

**Fig. 2:** Using the provider Helm chart through an umbrella

### Updating dependencies — the easy part

Most teams use tools like [Renovate](https://github.com/renovatebot/renovate) to update dependencies, run tests, render the resources, and check what has changed on the provider side.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*FUhPAo91ujv6lUDzIXUvZg.png)

**Fig. 3**: Updating dependencies with tools like Renovate

This part is easy.  
If the provider changes something, you can see how it affects your umbrella chart.  
If a variable or config is removed, the templating fails.  
If something that was opt-in becomes opt-out, you notice missing resources.

All good so far.  
Now let’s look at the reality.

### Reality check: cluster specific overlays

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*-6w_m5AKursAqx3v-LwdIg.png)

**Fig. 4:** Each cluster often needs dedicated values

If you manage cattle, not pets, you will have multiple overlay `values.yaml` files for every cluster.  
You might set Let’s Encrypt staging or production server, emails per team, or specific domain filters for External DNS.

The complexity increases.

From the simple:

helm template cert-manager .

to something like:

helm template cert-manager . --values ../../cluster-x/cert-manager/values.yaml

It becomes harder to test, but still manageable.

### And then came Argo CD

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*N-V09qaczu-M-OfthLhS6A.png)

**Fig. 5:** Argo CD takes over the Helm templating process

Now we let [Argo CD](https://argo-cd.readthedocs.io/en/stable/) handle it.  
We add logic to an engine that should only sync. Remember the four GitOps principles. I don’t recall “templating” being one of them.

and Argo CD does not just run:

helm template ...

It runs something like:

helm template . \  
  --name-template cert-manager \  
  --namespace cert-manager \  
  --kube-version 1.33 \  
  --values <path>/managed-service-catalog/helm/cert-manager/values.yaml \  
  --values <path>/customer-service-catalog/helm/pe-org/cert-manager/values.yaml \  
  --include-crds

If you test properly, you might simulate this with tools like [argocd-diff-preview](https://github.com/dag-andersen/argocd-diff-preview) for more realistic results.

### The patchwork monster

If you run on-prem or at the edge, you often need to patch values that cannot be overwritten or better say “patched” because Helm charts are too static.

Remember the change from [_PodSecurityPolicy (PSP)_ to _PodSecurityStandard (PSS)_](https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/) in Kubernetes (dep. in 1.21, removed 1.25).  
Many charts did not even have proper SecurityContext options.

Sure, you can fork the Helm charts, extend it, keep it in sync, and blow out your tests. Or. Or use [Kustomize](https://docs.plural.sh/examples/continuous-deployment/kustomize-inflate-helm) instead. (Not a recommendation — just how reality looks.)

You add another layer to template Helm charts and patch them.  
Because obviously, another abstraction layer will fix it.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*kVMBHzvYuhnwk-8SdqcXZQ.png)

**Fig. 6:** Adding a Kustomize layer on top of Helm — do not do this

[Argo CD can also handle this par](https://argo-cd.readthedocs.io/en/stable/user-guide/kustomize/)t, because we force the small GitOps engine into something it was never meant to be.

It reminds me of the movie:  
“Don’t Ever Feed Him After Midnight”  
GitOps was cute until it wasn’t.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*iHQEErFS9r57kRcD1BaVJA.png)

**Fig. 7:** Overloading the GitOps engine — one engine for every use case

And this is the point where GitOps starts to get ugly and begins to crumble.

## GitOps at scale means sprawl.

We are not even talking about [App of Apps](https://www.cncf.io/blog/2025/10/07/managing-kubernetes-workloads-using-the-app-of-apps-pattern-in-argocd-2/), [ApplicationSets](https://argo-cd.readthedocs.io/en/latest/user-guide/application-set/), or how many repositories hold your charts and values in detail.  
This is the reality of GitOps at scale: it’s configuration sprawl, not a pattern.

Beyond the challenge with Helm, you will having something like this in the enterprise area (if not just managing 2–3 clusters, stages):

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*1c60bgc8kfqCbLTNo1-HUQ.png)

**Fig. 8:** What Enterprise GitOps Can Look Like — Just a Snapshot

and this is where it starts to get interesting. Especially if something breaks.

### When it breaks

Now imagine something goes wrong.  
In production.  
And the most senior person is not there. And even if they were, it would not change much.

Every minute counts.  
Argo CD throws an error that has nothing to do with the actual problem.  
Maybe it is just a typo in a _ConfigMap_, something you could fix in seconds with `kubectl edit`.  
But you can’t. Because GitOps. It’ll reconcile and stubbornly try to reach the desired state. Don’t even think about disabling auto-sync — you can, sure, but during a multi-prod outage? It won’t save you.

Also, every company builds its own level of abstraction or chaos.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*acSQgsMTdH9VGohKAyYO6Q.png)

**Fig. 9:** Different companies, different setups, different levels of chaos

To fix it, you check dependencies in the umbrella chart, fetch overlay values for the affected cluster, run Kustomize, realize the issue is in Helm, go back, template again, fix, commit, and wait for [CI](https://about.gitlab.com/ebook-ultimate-guide-ci-cd/?utm_medium=cpc&utm_source=google&utm_campaign=eg_emea_dmp_x_x_en_gitlab_search_nb_singleappci-usecase_emea_broad&utm_content=eg_global_cmp_gated-content_depflex_en_guidecicd_x_x&utm_term=cicd&_bt=740261293420&_bk=cicd&_bm=b&_bn=g&_bg=145507599496&gad_source=1&gad_campaignid=18463394777&gbraid=0AAAAADcJCbc5tOY48jovm5gl067X44hOF&gclid=Cj0KCQjwgpzIBhCOARIsABZm7vFDNpImeipU3a_6C9who0ZQrgvwIC2pugAgFLFSHc_CZVigTA0tCcgaAhXHEALw_wcB) tests.  
Congratulations. You just fixed a typo. It only took an hour. In one Environment? What about the other environments?

### The root cause

Helm is great for package management.  
I never said that out loud.

Helm as a templating engine? Not a good idea.  
Helm as a templating engine and Argo CD as the executor? Bad idea. Actually, really bad.  
It works, but would you using your package manager to rewrite configuration files or deploy custom applications on your Linux distro? That is where it gets messy.

This happens because we mix Configuration as Code (CaC) with actual code.

### What GitOps should be versus what it became

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*EQUtTptfun50Bwo05i_rOg.png)

**Fig. 10:** The GitOps engine should only apply manifests and keep them in sync

But instead, we now do this:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*fSPvtXNPSUH-J7SQkwxwVg.png)

**Fig. 11:** The GitOps engine mixes Configuration as Code with code execution

[Configuration as Code](https://octopus.com/blog/config-as-code-what-is-it-how-is-it-beneficial) = Helm charts  
Code = GitOps engine

And that’s the real challenge.

This is why tools like argocd-diff-preview or [Source Hydrator](https://argo-cd.readthedocs.io/en/latest/user-guide/source-hydrator/) from Argo CD were created. Taking YAML back under your control.  
They generate or hydrate resources so the GitOps engine can just sync.

It is about having a clear, visible desired state that actually matches what is in Git.

### Imagine now ClickOps in a good way

We went a bit too far with our GitOps journey.  
[Even Alexis Richardson, the creator of GitOps](https://schlomo.schapiro.org/2021/02/gitops-interview-alexis-richardson.html), now talks about config sprawl. Together with [Brian Grant](https://www.linkedin.com/in/bgrant0607/), and [Jesper Joergensen](https://www.linkedin.com/in/jesperfj/), they started working on this challenge.

That is where [ConfigHub](https://www.confighub.com/) was born.

## What is ConfigHub and why it will solve that problem

> **Note:** ConfigHub was created by Alexis Richardson (ex. Weaveworks, GitOps), Brian Grant (original Kubernetes lead architect), and Jesper Joergensen (ex. product lead for Heroku at Salesforce). When great minds come together, something interesting usually happens.

### Configuration as Data (CaD) — a paradigm shift

[Config-as-data](https://itnext.io/what-is-configuration-as-data-210b0c4be324) represents a paradigm shift from managing scattered [YAML](https://yaml.org/) files to treating configuration as structured data with schema validation, relationship modeling, and query capabilities.  
Instead of debugging template logic and overlay interactions, operations teams can directly inspect and validate the actual configuration values that will be applied to clusters.

As mentioned before, it makes sense to keep your Configuration as Code separated from Code.  
That idea is at the core of what ConfigHub defines.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*XRCZh9hq-jZmQSBYnu07eA.png)

**Fig. 12:** Do not overload your GitOps engine tool

### Infrastructure as Code vs Configuration as Data

> **Note:** for the purpose of this context (e.g., in relation to ConfigHub, where functionality overlaps), we will treat [**IaC (Infrastructure as Code)** and **CaC (Configuration as Code)** as synonyms](https://www.clouddefense.ai/difference-between-iac-and-cac/). Or CaC as subset of IaC.

**Configuration as Code (CaC)** is implemented in **Helm Charts** e.g. to centralize application settings (**values**) and dynamically populate resource templates, effectively abstracting the underlying Kubernetes manifests. Of course you can use it, to provision infrastructure. Then it becomes IaC.

[**Configuration as Data (CaD)**](https://docs.confighub.com/background/config-as-data/) is an approach that represents and manages system and application configuration as structured data rather than code, enabling tools to read, write, and reconcile configuration automatically like any other data source.

ConfigHub follows a versioned, database-driven approach to achieve this separation and store Configuration-as-Data (CaD) within its database.

Take a look at the following example to better understand the difference between CaC and CaD.

**CaC with Helm**

containers:  
  - image: {{ .Values.deployment.main.image }}

You then add a parameter to the `values.yaml` file for that environment:

deployment:  
  main:  
    image: ghcr.io/myorg/myapp:release12345

**CaD with plain YAML**

With Configuration as Data, the full standard Kubernetes Deployment YAML is stored, including the image:

apiVersion: apps/v1  
kind: Deployment  
metadata:  
  name: myapp  
spec:  
  template:  
    spec:  
      containers:  
      - name: main  
        image: ghcr.io/myorg/myapp:release12345

> Note: `[cub](https://docs.confighub.com/developer/cli/cub-overview/)` [](https://docs.confighub.com/developer/cli/cub-overview/)is the command-line tool for using ConfigHub.

You can change the image with a simple command:

cub function invoke --unit myapp set-image main "ghcr.io/myorg/myapp:$TAG"

In ConfigHub, configuration is represented, stored, and managed as data. It is serialized using standard data formats such as YAML and stored in a database within ConfigHub.  
Code that operates on configuration is separate from the data, and the data is the source of record.  
Configuration data is not parameterized. It contains literal values for every field in the configuration.

This is in contrast to Infrastructure as Code (IaC) or CaC, which represents configuration as code-like templates managed in Git and deployed through CI pipelines or GitOps tools.  
ConfigHub enables configuration updates and queries via API rather than code-based tools.  
Configuration data is not stored in Git for the same reason database tables are not stored as template code that generates CSV files.

### ConfigHub relies on fully rendered (WET) config

> **Note:** [WET is the opposite of DRY](https://medium.com/@fulton_shaun/dry-vs-wet-code-the-simple-rule-that-separates-clean-code-from-chaos-a3e919dbad92). DRY tries to reduce redundancy, while WET (Write Every Time) stores full configuration for each variant.

With ConfigHub, the configuration of every variant is stored independently in its native, fully rendered, WET form.  
There are no templates, variables, conditionals, loops, or generators that create configuration on the fly.  
Every value specific to an environment is stored literally.

In a Kubernetes deployment, ingress hostnames, environment variables, image tags, service dependencies, resource requests, and more are all stored directly in the YAML instead of being interpolated dynamically via template variables and input values.

This means the configuration is always ready to go.  
Make a simple edit to the config, and the live resources can be updated without running a complex CI/CD process.

Policy enforcement can check the configuration directly and report results immediately, unlike templated config that first needs rendering.  
Issues can also be fixed automatically by updating the configuration.

Each config is isolated. Changes affect only one environment with no risk of collateral damage.  
This is unlike changes to templates and values files that might affect many environments.

### [ConfigHub architecture](https://docs.confighub.com/background/architecture/)

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*jiRHTA1fOefu-lTq_BOL7Q.png)

**Fig. 13:** Overview of ConfigHub architecture

### [Multi-tenant SaaS core](https://docs.confighub.com/background/architecture/)

At its core, ConfigHub is a multi-tenant [SaaS](https://www.hostinger.com/tutorials/what-is-saas?utm_campaign=Generic-Tutorials-DSA-t2%7CNT%3ASe%7CLang%3AEN%7CLO%3ADE&utm_medium=ppc&gad_source=1&gad_campaignid=23129756374&gbraid=0AAAAADMy-haFEl1nUDjc7GbUgWh_Fh51Z&gclid=Cj0KCQjwgpzIBhCOARIsABZm7vHf3Z8fVGAe_aaAST-nV1tqqmZfDaXAxEMRFXbvfgZBurHTInRjL_kaAmqUEALw_wcB) following standard B2B SaaS practices.  
As a customer, you have an organization in ConfigHub where your team members collaborate.  
You can log in with your work account (Google Workspace or others) and set up [SAML](https://auth0.com/resources/ebooks/saml-authentication-explained?utm_source=google&utm_campaign=emea_dach_deu_all_ciam-all_dg-ao_auth0_search_google_text_kw_utm2&utm_medium=cpc&utm_id=aNK4z0000004IOrGAM&utm_term=saml-c&gad_source=1&gad_campaignid=16883988662&gbraid=0AAAAACmv60WYbY6KA3SB12B6VS63qc_ES&gclid=Cj0KCQjwgpzIBhCOARIsABZm7vFHPBATZ9kdBVDq3kaUq7yC84nIToIUpw8wGqBRqHGAGHTV-V5g3QoaAvhkEALw_wcB) or [OIDC](https://auth0.com/intro-to-iam/what-is-openid-connect-oidc)-based [SSO](https://www.cloudflare.com/en-gb/learning/access-management/what-is-sso/).

The ConfigHub cloud instance stores all configuration data and orchestrates all access and change workflows.  
All changes to configuration data are versioned in a database, so you can track what caused them, compare to past revisions, and revert when needed.

ConfigHub does not store credentials or secrets for your infrastructure.  
These are stored externally and only accessed by Workers.

### Web UI, CLI, and API

ConfigHub provides both a Web UI and a [CLI.](https://docs.confighub.com/get-started/setup/#install-the-cli)  
All functionality is available in both, so you can choose the most convenient tool.  
Naturally, ConfigHub also has an [API.](https://docs.confighub.com/developer/api-reference/overview/)

### [Workers](https://docs.confighub.com/background/entities/worker/) — the GitOps engine

A Worker is a software process running in your infrastructure that connects to ConfigHub.  
It performs infrastructure operations and executes function logic.  
It is similar to a Kubernetes GitOps [Operator](https://www.cncf.io/blog/2022/06/15/kubernetes-operators-what-are-they-some-examples/) or CI runner.  
The Worker runs inside your cluster.

A closer look at the core components shows the five most important parts of ConfigHub.

### The core of ConfigHub

At the Top of spaces, you have your organization, which you should be already familiar from other solutions.

- [**Spaces:**](https://docs.confighub.com/background/entities/space/) Used to group and isolate configurations within an organization. They serve as the main unit for access control and collaboration, allowing different policies across development, testing, and production.
- [**Units:**](https://docs.confighub.com/background/entities/unit/) A Config Unit stores versioned configuration data for a single toolchain type (e.g., Kubernetes YAML or [OpenTofu](https://opentofu.org/)). It manages the full lifecycle of configuration — from creation and revision tracking to applying and refreshing live resources — while ensuring isolation and control.
- [**Functions**](https://docs.confighub.com/background/entities/function/)**:** Executable pieces of code that operate on configuration data within Config Units. They can be readonly, mutating, or validating, and extend ConfigHub’s automation capabilities.
- [**Targets:**](https://docs.confighub.com/background/entities/target/) Represent deployment destinations such as Kubernetes clusters or cloud accounts. They abstract connection details and credentials, enabling Workers to manage infrastructure access securely.
- [**Workers:**](https://docs.confighub.com/background/entities/worker/) External processes connecting to ConfigHub to execute functions and manage infrastructure bridges. They communicate with ConfigHub through a secure, persistent connection.
- [**Bridge:**](https://docs.confighub.com/background/entities/bridge/) connects ConfigHub with live resources across Kubernetes, AWS, Azure, GCP, and other systems by translating ConfigHub actions — Apply, Destroy, Refresh, and Import — into the correct API calls and relaying operation events and status back to ConfigHub.  
    Bridges run inside **Workers**, which handle communication, authentication, and connectivity, while the Bridges focus solely on resource logic; ConfigHub provides prebuilt Worker–Bridge binaries (e.g., for Kubernetes) and supports custom Bridge implementations in Go.

It will look like this:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*n6eJclzNPsTVD1QYhECOFQ.png)

**Fig. 14:** High-level overview of the core ConfigHub components

### A high-level example

> **Note:** The demo and CLI are still under active development, so if you want to follow along, make sure to check the [latest version of the repository](https://github.com/confighub/examples/tree/main/global-app) and CLI for any recent changes.

Now let’s get practical. We will not reveal too much (private beta), but we can give you a taste of what has been tested.

This example from ConfigHub itself demonstrates how to use ConfigHub to manage a micro-service application deployed in multiple environments.

### Scenario

The application has four components:

- React frontend
- Go backend
- Ollama container
- Postgres database

Environments:

- QA
- Staging (US, EU, Asia)
- Production (US, EU, Asia)

Total: seven live environments plus a base configuration that is covered later.

### ConfigHub layout

The example is divided into app and infrastructure parts.  
It illustrates how app configuration interacts with infrastructure configuration.

A Kubernetes namespace is used to simulate infrastructure.  
In reality, this would be a separate cluster per environment.

In a real world example, spaces are used for each layer to allow independent governance per region.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*56EVhUZ-7wda1RdjbXpFZA.png)

**Fig. 15:** App space hierarchy from the demo

This diagram shows the infrastructure config:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*ui3XBs1EANRBhP10cDThcA.png)

**Fig. 16:** Infra hierarchy from the demo

The infrastructure does not need to flow from staging to production; all environments are downstream from a base config.

### [Base units](https://docs.confighub.com/guide/variants/#base-units)

> **Note:** [Base Units are the building blocks](https://docs.confighub.com/background/concepts/variant/) to manage configuration for different environments.

Example: [ingress-nginx](https://github.com/kubernetes/ingress-nginx)  
There is both an `nginx-base` and an `nginx` unit.

This layout uses ConfigHub’s **clone upgrade** feature to combine **external updates** with local changes. You can think of it like having a **Kustomize** [base with overlays](https://github.com/kubernetes-sigs/kustomize?tab=readme-ov-file#2-create-variants-using-overlays) that patch values specific to each environment’s needs.  
You can upgrade the base unit with a new YAML version, inspect the diff, [and then **upgrade downstream**](https://medium.com/itnext/how-are-variants-managed-in-confighub-061bc2f89cff) without overwriting local modifications.

After the units have been deployed, you can list the infrastructure configuration with:

# "hug-paw-" is a random prefix used to generate unique names  
# "infra" refers to the Space in which the Units reside  
  
cub unit tree --space "hug-paw-infra"

**Output**

# 10 units in the space hug-paw-infra.   
NODE                       SPACE            STATUS     UPGRADE-NEEDED    UNAPPLIED-CHANGES    APPLY-GATES   
└── nginx-base             hug-paw-infra    NotLive                                           None             
    └── nginx              hug-paw-infra    NotLive    No                                     None             
└── ns-base                hug-paw-infra    NotLive                                           None             
    ├── ns-us-staging      hug-paw-infra    NotLive    No                                     None             
    ├── ns-asia-staging    hug-paw-infra    NotLive    No                                     None             
    ├── ns-asia-prod       hug-paw-infra    NotLive    No                                     None             
    ├── ns-eu-prod         hug-paw-infra    NotLive    No                                     None             
    ├── ns-eu-staging      hug-paw-infra    NotLive    No                                     None             
    ├── ns-us-prod         hug-paw-infra    NotLive    No                                     None             
    └── ns-qa              hug-paw-infra    NotLive    No                                     None

You can already see the power of ConfigHub: one command acts like a query showing all units of the “hug-paw-infra” space in a structured tree view.

If you are taking a look at the UI, you will find the same resources.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*AnavKbszR4qncGuuhTFsAw.png)

**Fig. 17:** ConfigHub UI — spaces view

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*fYtSQ5TNoruyFlNGl7dvoQ.png)

**Fig. 18:** ConfigHub UI — infra space view

As you can see, the same ten units mentioned in the comment are also present inside the spaces. But we prefer the CLI view — it’s faster and gives a nice overview of how the node graph is built up.

We’ve now created the logic and stored the manifest in the ConfigHub database. Next, we’ll deploy it on Kubernetes via a Worker.

### Deploying a Worker

Create a local cluster and connect it to ConfigHub:

#create kubernetes in docker cluster  
kind create cluster --name hug-paw-infra   
  
#create worker in confighub  
cub worker create cluster-worker --space hug-paw-infra  
  
#bridge worker with the cluster or deploy worker on the cluster  
cub worker install cluster-worker \  
    --space hug-paw-infra \  
    --env IN_CLUSTER_TARGET_NAME=cluster-target \  
    --export \  
    --include-secret \  
  | kubectl apply -f -

Check namespaces:

kubectl get ns

Output:

NAME                 STATUS   AGE  
confighub            Active   2m41s  
default              Active   2m47s  
kube-node-lease      Active   2m47s  
kube-public          Active   2m47s  
kube-system          Active   2m47s  
local-path-storage   Active   2m43s

Check pods:

kubectl get pods -n confighub

Output:

NAME                              READY   STATUS    RESTARTS   AGE  
cluster-worker-57f4fbc788-4w54d   1/1     Running   0          3m44s

Apply infra to the target cluster:

cub unit apply --space hug-paw-infra --where "Labels.targetable = 'true'"  
  
Awaiting 8 apply operations...

Cluster state:

kubectl get ns

Output:

NAME                 STATUS   AGE  
asia-prod            Active   3m17s  
asia-staging         Active   3m17s  
confighub            Active   7m54s  
default              Active   8m  
eu-prod              Active   3m17s  
eu-staging           Active   3m17s  
ingress-nginx        Active   3m16s  
kube-node-lease      Active   8m  
kube-public          Active   8m  
kube-system          Active   8m  
local-path-storage   Active   7m56s  
qa                   Active   3m17s  
us-prod              Active   3m17s  
us-staging           Active   3m17s

Now you can also see “Bridge Worker” in the infra space:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*wciNYrjkkHGTz9XgPUnkJg.png)

**Fig. 19:** ConfigHub UI — Spaces view, connected Worker

We mentioned its saved in versioned DB behind the scenes. You can, for example, edit the `Namespace` resource and add a label directly in the UI.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*pB_7KEXx-MVuIXz1hcM0SQ.png)

**Fig. 20:** ConfigHub UI — Change in unit ‘nginx’,

After editing a unit in the UI, the change is versioned like in Git. You’ll also be able to submit a change message, just like you’re used to in Git.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*gnK6dXrK4UloLpZpORCDfg.png)

**Fig. 21:** ConfigHub UI — change in unit nginx and new head revision

Result in the namespace on the target cluster after applying the change:

apiVersion: v1  
kind: Namespace  
metadata:  
  annotations:  
    config.k8s.io/owning-inventory: 3aa82138-71d5-494a-bb7a-0e51f3afcced-nginx  
  creationTimestamp: "2025-10-30T23:34:31Z"  
  labels:  
    app.kubernetes.io/instance: ingress-nginx  
    app.kubernetes.io/name: ingress-nginx  
    kubernetes.io/metadata.name: ingress-nginx  
    useless: change <-- CHANGED HERE <------------------------  
  name: ingress-nginx  
  resourceVersion: "1982"  
  uid: b0cdcc12-52fd-4ca0-8603-3ba2d4fa894b  
spec:  
  finalizers:  
  - kubernetes  
status:  
  phase: Active

You can also roll back changes at any time. Like you would revert a commit.

Hopefully, you’re starting to see where this is going.  
Take a look at another snippet showing how you can update and promote configurations.

### Updating and promoting configurations — Developer View

Let’s say we now want to promote a new version of our application images — starting with QA.

**Bump QA**

To bump the version of our application images in QA, we use a ConfigHub [Function called set-image-reference](https://docs.confighub.com/background/entities/function/). This is a mutating function maintained by ConfigHub — one of [dozens of built-in functions available](https://docs.confighub.com/developer/cli/cub_function_list/). It accepts inputs like container name and image reference, and can be executed on a Unit using the cub CLI.

# bump versions frontend  
cub run set-image-reference --container-name frontend --image-reference :1.1.5 --space hug-paw-qa  
  
# bump versions backend  
cub run set-image-reference --container-name backend  --image-reference :1.1.5 --space hug-paw-qa  
  
# apply changes  
cub unit apply --space hug-paw-qa

**Promote to next environment  
**Remember, we have three stages (QA, Staging, Production) and three regions (US, EU, Asia). Next, we promote our changes from QA to Staging in the US region.

Next, let’s check the upgrade status of other environments:

cub unit tree --node=space --filter hug-paw/app --space "*" --columns Space.Slug,UpgradeNeeded,UnappliedChanges  
  
OUTPUT LIKE:  
  
NODE                                     UNIT        SPACE                       UPGRADE-NEEDED    UNAPPLIED-CHANGES  
└── chubby-paws-base                     backend     chubby-paws-base  
    └── chubby-paws-qa                   backend     chubby-paws-qa              No  
        ├── chubby-paws-asia-staging     backend     chubby-paws-asia-staging    Yes  
        │   └── chubby-paws-asia-prod    backend     chubby-paws-asia-prod       No  
        ├── chubby-paws-eu-staging       backend     chubby-paws-eu-staging      Yes  
        │   └── chubby-paws-eu-prod      backend     chubby-paws-eu-prod         No  
        └── chubby-paws-us-staging       backend     chubby-paws-us-staging      Yes

Because the Frontend and Backend Units in us-staging are downstream of QA, ConfigHub marks them as “Upgrade-Needed”.

Preview the changes with a dry-run:

cub unit update --dry-run --patch --upgrade --space hug-paw-us-staging

Then apply the upgrade:

cub unit update --patch --upgrade --space hug-paw-us-staging

**Promote everywhere**

When we are confident in a new version, rolling it out everywhere takes just a few steps. We start by upgrading all environments:

cub unit update --patch --upgrade --filter hug-paw/app --space "*"

As you can see, it becomes surprisingly easy to make changes across environments — combining the CLI with a query-like language using parameters and regex. You get a true [_Single Pane of Glass_](https://www.ibm.com/think/topics/single-pane-of-glass) over all environments, with just one command. This is possible, because it combines hydrated manifest (WET), stored in a versioned database and this allows ConfigHub to provide “**Configuration as Data**”.

If you’re wondering _“What about Helm operations?”_ — ConfigHub covers those as well, [check this out](https://github.com/confighub/examples/tree/main/helm-platform-components), but we won’t go into more code here.

Let’s take a quick look at the advantages we’ve already seen.

### Advantages

First of all, you can keep your existing workflow — you just need to add a small piece, like a generator or hydrator, that creates the manifests from your abstraction layer. Maybe you’re already hydrating resources (like many companies do), in which case the transition feels more like an import into ConfigHub.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*HSZLV6GCcLhZv9VT4-h_dA.png)

**Fig. 22:** Combining Helm, GitOps, and ConfigHub (to be presented at KubeCon NA)

Hopefully it became clear during the demos that you can now actually _see_ what’s happening across all your environments — all through a single pane of glass.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*iOBV_lLYuN9J1GxeV_OwUQ.png)

**Fig. 23:** Get a Better Overview of Your Environments

But you can also control your entire environment through that same single pane of glass and single point of control — no more jumping between repos or folders.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*ns5VXGRMgmXV9rjU6axBww.png)

**Fig. 24:** Not Just What Happens Across Your Environments — But Taking Control of It

ConfigHub is more than just a smart database for storing configuration as data. It gives you the ability to define and automate complete workflows.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*xQPvfKQGeUu8GB4LPGVOfA.png)

**Fig. 25:** Combine ConfigHub with your External Systems for Pre-Apply Checks

For example, you can combine ConfigHub with external systems to run **pre-apply checks**. You might start with a base configuration (for example, replicas = 1), then run mutation tasks to adjust it for production (set replicas = 3). External systems can then perform compliance or security checks, or modify the resource if something is missing. Once approved, the final configuration can be deployed to the target cluster.

You can also use **ConfigHub Native Workflows** to automate processes directly inside ConfigHub.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*xczO0rC27nrjrMdYBqj12w.png)

**Fig. 26:** Create ConfigHub Native Workflows

Triggers can be defined based on events, which then call functions to meet your requirements and apply the approved configuration to the target cluster.

This is possible because [ConfigHub provides an SDK](https://github.com/confighub/sdk) for building custom API clients, workers, and integrations.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*QyOoZxNF__QM4VpmBdDbGg.png)

**Fig. 27:** [ConfigHub SDK](https://github.com/confighub/sdk)

But we see more benefits, here are just few of them:

- Works with existing complex workflows, no refactor needed
- Single pane of glass for all environments and Single point of control (CLI, UI, API)
- Versioned database with full history, diffs, and rollback
- Query language instead of folder hopping
- Integrates with existing IDPs or Kubernetes based platforms
- Run CI workflows where your configuration lives. Since ConfigHub Functions can be written in Go, you can wrap tools like [Kyverno](https://kyverno.io/docs/testing-policies/) in custom functions that run on every configuration update.
- ConfigHub moves beyond a simple data sink. By utilizing the ConfigHub SDK, you can build custom workers, API clients, and other tools to manage your configuration.

**ConfigHub** is **not** an IDP, nor does it replace one. ConfigHub **complements** your IDP. It serves as a centralized configuration pane of glass, powered by an API that is easily integrated into your existing IDP.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*vW8y2TLQW-WjvIdiBCW79Q.png)

**Fig. 28:** ConfigHub — more than just a Config as Data Sink!

Right now, there aren’t any major downsides — apart from the fact that ConfigHub is still under active development.

Some might raise an eyebrow about potential **vendor lock-in** — after all, ConfigHub stores **configuration data** in its managed database rather than in Git. But in practice, everything is still **just rendered YAML**. You can export it at any time and re-integrate it into your Git-based workflow if you ever decide to move away. So it’s not a real lock-in.

You just need to make one shift in perspective: start seeing configuration as data, not as code.

## Wrap Up and Next Steps

If you got the idea so far, you probably understand why your **Internal Developer Platform (IDP)** needs solutions like **ConfigHub** — especially when you are running [**GitOps at scale**](https://blog.devops.dev/gitops-at-scale-69639c9a3dd7).

Instead of doing this, which many of you might recognize:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*vyo13cvvbxXhY-5vjRgYqA.png)

**Fig. 29:** A typical enterprise setup I often see in many projects — sometimes even with Kustomize added on top

You can achieve something much simpler and more efficient, after you import your manifest into confighub:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*vW8y2TLQW-WjvIdiBCW79Q.png)

**Fig. 30:** ConfigHub — The solution for your Internal Developer Platform?

The fact that we already think about using a central repository:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1050/1*0zsoF9IYWPbpFDwxm-vsRw.png)

**Fig. 31:** We already try to centralize infrastructure to manage our stack better

…shows why your Internal Developer Platform truly needs solutions like **ConfigHub!!**