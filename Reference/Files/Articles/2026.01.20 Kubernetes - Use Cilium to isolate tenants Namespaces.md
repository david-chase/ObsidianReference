#cilium #cni #k8s #networking 

![](https://miro.medium.com/v2/resize:fit:1050/1*ydct80GQor6KciNMlbU_jw.jpeg)

_Having a Kubernetes cluster with multiple “tenants” usually starts with RBAC: each user gets their own namespace and is only allowed to create/read/update resources there. In my previous article about Kubernetes authentication with Keycloak OIDC, I built exactly that setup. RBAC worked well, but it also revealed an important gap:_ **_RBAC limits what users can do with the API, but it does not stop Pods from talking over the network to workloads in other namespaces._**

## Kubernetes Authentication. OIDC with Keycloak

### Kubernetes Authentication. OIDC with Keycloak Having a Kubernetes cluster means you eventually need real user access…

fenyuk.medium.com



](https://fenyuk.medium.com/kubernetes-authentication-oidc-with-keycloak-47c4d02133e2?source=post_page-----52eb5e877459---------------------------------------)

_This article continues the same story, but shifts focus from “Who can do what?” to “Who can talk to whom?”. We’ll keep the same two-tenant scenario (two users, two namespaces, separate workloads), and we’ll add_ **_real network isolation_** _so that Pods in_ `_tenant-user1_` _cannot reach Services/Pods in_ `_tenant-user2_` _and vice versa._

To enforce that isolation, the plan is to leverage [_Cilium_](https://cilium.io/) as the networking and policy engine. _Cilium_ runs an agent on each node and enforces Kubernetes network policy using [**_Linux eBPF_**](https://ebpf.io/), which makes policies label/identity-driven and very practical for dynamic Kubernetes workloads. In addition, we can use **Hubble** (Cilium’s observability layer) to _see_ network flows and understand exactly why a request was allowed or denied — which is extremely helpful when you start tightening policies.

Initial step is to create [_kind_](https://kind.sigs.k8s.io/) cluster with two nodes and custom configuration:

**#5:** disable default CNI plugin, which has too open settings and which is to be replaced with _Cilium’s_ CNI;

**#8..#12:** Cluser has two nodes;

**#14..#24:** as described in detail in [the previous article](https://fenyuk.medium.com/kubernetes-authentication-oidc-with-keycloak-47c4d02133e2), Cluster uses local Keycloak server as OIDC provider. This part is not important in the current chapter, so this configuration is optional.

Next is to install _Cilium_ Helm chart:

In half of minute the deployment is finished:

Two test users have individual Namespaces, which need to be created:

and network access there needs to be explicitly set up with [_Network Policy_](https://docs.cilium.io/en/stable/network/kubernetes/policy/#k8s-policy):

There blocks have inline explanation, please reference the code snippet itself and apply the file with _kubectl apply -f tenant-isolation.yaml_ command.

To test access from both namespaces, **test server Pod** needs to be run and expose an IP port:

start test Pod in ns tenant-user1

Make sure that _httpd Pod_ is accessible within the same Namespace:

**#1:** start **curl Pod** in same Namespace;

**#2:** **curl** to _DNS httpd_ (full alias [http://httpd.tenant-user1.svc.cluster.local:80](http://httpd.tenant-user1.svc.cluster.local/));

**#4..#35:** positive response from _httpd Pod_.

Time to check that the other tenant, who can only create a resource in Namespace **tenant-user2**, can not communicate with Namespace **tenant-user1.**

**#1:** start **curl Pod** in different **tenant-user2** Namespace;

**#2:** try to talk with the forbidden resource, running in the different Namespace and belonging to the different tenant;

#4..#10: expected failure.

Last snipped proved the fact that _Cilium’s Network policies_ isolate Namespaces one from other. This configuration strengthens [previous chapter setup](https://fenyuk.medium.com/kubernetes-authentication-oidc-with-keycloak-47c4d02133e2), which uses [Kubernetes RoleBindins](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) to limit tenants to own Namespaces and utilizes [Keycloak](https://www.keycloak.org/) as [OIDC provider](https://openid.net/) for authentication.

There is one quick-to-fix weakness in the above setup. From within Namespaces it is impossible to connect to the common internet, because _NetworkPolicy_ denies it. The following test demonstrates it:

_Cilium_ has [CiliumNetworkPolicy](https://docs.cilium.io/en/stable/network/kubernetes/policy/#ciliumnetworkpolicy) which can easily fix the problem:

**#5, #8..#10:** for **tenant-user1** Namespace apply special **world egress** policy;

**#15, #19..#21:** same for **tenant-user2** Namespace.

Same command now reaches internet:

_This completes the missing layer that RBAC alone can’t cover:_ **_network reachability_**_. In the_ [_previous part of this series_](https://fenyuk.medium.com/kubernetes-authentication-oidc-with-keycloak-47c4d02133e2)_, Keycloak OIDC established clean API-level separation for tenants. In this part, Cilium Network Policies add the enforcement needed for_ **_real tenant isolation on the wire_**_. From this baseline, the setup can be extended toward a more production-like model: controlled internet egress, a “shared services” namespace (DNS, observability, ingress), and progressively tighter rules validated with Hubble._