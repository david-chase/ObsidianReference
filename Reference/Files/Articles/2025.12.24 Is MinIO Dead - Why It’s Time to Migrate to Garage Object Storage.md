#k8s #storage #s3 #garage #howto #tutorial 

**Is the era of default, free S3-compatible storage changing?**

Two days ago, a quiet but seismic shift occurred in the open-source storage landscape. Without a press release or a grand announcement, a single commit to the official MinIO repository changed the project’s status to **“Maintenance Mode.”**

For thousands of DevOps engineers and SREs running MinIO in production, this commit changed reality instantly. The repository now explicitly states that the codebase is in a maintenance-only state, no new features will be accepted, and critical security fixes will only be evaluated on a “case-by-case” basis. The suggested solution? Move to the commercial product, MinIO AIStor.

If you rely on community support and regular security patches, that “case-by-case” clause is a significant compliance red flag. But the open-source ecosystem is resilient.

In this post, we look at **Garage**, a robust, open-source S3-compatible object storage solution developed by Deuxfleurs. We will walk through deploying Garage on Kubernetes to replace your unmaintained storage stack.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:700/1*j8oxP8SEh552Ce3BSnuPtw.png)

Mini io To Garage

## Why Garage?

Garage is designed for self-hosting. It is an S3-compatible object store that focuses on reliability, scalability, and efficiency. Unlike some storage solutions that require massive resources, Garage is lightweight and handles “split-brain” scenarios gracefully, making it highly resilient for distributed setups.

Let’s get your new storage backend running.

## Step 1: Install Garage on Kubernetes

We will use Helm to deploy Garage. First, clone the repository to get the necessary charts.

Bash

git clone https://git.deuxfleurs.fr/Deuxfleurs/garage  
cd garage/scripts/helm

You can deploy with the default configuration using the command below. This creates a dedicated namespace `garage-system` and installs the release.

Bash

helm install \  
  --create-namespace \  
  --namespace garage-system \  
  garage ./garage

_Note: For production environments, you should create a_ `_values.override.yaml_` _file to customize your deployment (e.g., setting replica counts to 3 for high availability)._

## Step 2: Verify the Deployment

Once Helm finishes, ensure the pods are running. You can check the internal status of the Garage node using the binary inside the pod:

kubectl exec --stdin --tty -n garage-system garage-0 -- \  
  ./garage status

## output   
  
==== HEALTHY NODES ====  
ID                Hostname  Address           Tags  Zone  Capacity          DataAvail  
3cc4c9f06678f40d  garage-0  10.42.2.134:3901              NO ROLE ASSIGNED

**What to look for:** You should see a `HEALTHY NODES` section listing your node ID, hostname, and address. Note down **ID** is needed for **step 3**

## Step 3: Configure the Cluster Layout

Garage has a unique concept called a “Layout.” You must explicitly inform the cluster about the disk space available on each node and which “zone” (datacenter or rack) they belong to.

In this example, we are configuring a single node in a zone named `dc1` with a capacity of `8G`.

- **Get your Node ID:** From the `status` command in Step 2, copy the Node ID (e.g., `3cc4c9f06678f40d`).
- **Assign the layout:**

kubectl exec --stdin --tty -n garage-system garage-0 -- \  
  ./garage layout assign \  
  -z dc1 \  
  -c 8G \  
  <YOUR_NODE_ID>

- **Apply the layout:** Changes in Garage must be explicitly applied to take effect.

kubectl exec --stdin --tty -n garage-system garage-0 -- \  
  ./garage layout apply --version 1

## Step 4: Create a Bucket and API Keys

Now that the storage layer is active, let’s create a bucket (e.g., for a NextCloud deployment or general storage) and the credentials to access it.

**Create the bucket:**

kubectl exec --stdin --tty -n garage-system garage-0 -- \  
  ./garage bucket create demo-bucket

**Create an API Key:** Unlike AWS IAM where keys are users, in Garage, keys are independent entities that can be assigned to buckets.

kubectl exec --stdin --tty -n garage-system garage-0 -- \  
  ./garage key create demo-bucket-key

_Important:_ The output will display your `Key ID` (Access Key) and `Secret key`. **Save these immediately**, as they will not be shown again.

**Grant Permission:** Bind the key to the bucket with full permissions (Read, Write, Owner).

kubectl exec --stdin --tty -n garage-system garage-0 -- \  
  ./garage bucket allow \  
  --read \  
  --write \  
  --owner \  
  demo-bucket \  
  --key demo-bucket-key

## Step 5: Verify Connectivity with AWS CLI

To verify that Garage is acting correctly as an S3 endpoint, we will use the standard `awscli`.

**1. Configure a profile:**

aws configure --profile garage-demo

Enter the credentials you generated in Step 4. For the region, you can use `us-east-1` (standard for S3 compatibility), and output format `json`.

**2. Test Upload and List:**

We need to point the AWS CLI to the Garage service. First, find your service IP:

kubectl get svc -n garage-system garage

Now, upload a simple text file:

echo "Hello Garage!" > hello_garage.txt  
  
aws s3 cp hello_garage.txt s3://demo-bucket/ \  
  --profile garage-demo \  
  --endpoint-url http://<garage_svc_lb_ip>:3900

If successful, you will see an upload confirmation. You can list the files to be sure:

aws s3 ls s3://demo-bucket/ \  
  --profile garage-demo \  
  --endpoint-url http://<garage_svc_lb_ip>:3900

## Conclusion

The shift of MinIO to maintenance mode is a reminder that open-source strategies must remain agile. While MinIO’s legacy is undeniable, projects like **Garage** offer a modern, community-driven path forward for those needing strict open-source adherence without the enterprise price tag.